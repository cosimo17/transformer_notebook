{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ea8479a",
   "metadata": {},
   "source": [
    "# 1.Transformer介绍\n",
    "Transformer是谷歌于2017年提出的一个用于解决序列生成任务的网络模型。自提出至今，Transformer的论文已经被引用七万多次，足以见其影响力之大。Transformer是第一个纯粹基于自注意力机制的序列模型，彻底抛弃了之前对RNN或者CNN的依赖。Transformer架构简洁，设计高度模块化，规模易拓展且利于并行计算，得益于这些优势，Transformer在NLP领域大放异彩，几乎所有NLP任务上的state-of-the-art模型都是基于Transformer架构，此外，随着VIT,SWIN Transformer的提出，Transformer也被应用到了计算机视觉领域，并且展示出了极大的潜力。\n",
    "\n",
    "在本篇教程中，我将解释Transformer中的一些核心概念，并将手把手地教你实现一个Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1a0da1",
   "metadata": {},
   "source": [
    "# 2.重要的核心概念\n",
    "### 2.1 什么是Embedding\n",
    "Embedding，也叫词嵌入技术，通过将语言中的单个字或者词映射为一个n维的数值向量，从而便于计算机进行计算，便于神经网络进行处理。\n",
    "\n",
    "针对某种特定语言中字词的编码问题，很容易想到的一种编码方式就是one-hot编码。很显然，one-hot编码存在两个缺点，第一是one-hot的编码效率低下，大部分的码位都是0，只有一位是1，容易浪费计算机的存储空间，第二个更严重的缺点是，one-hot方式编码下，每一个编码与其它编码都是垂直正交的，而我们知道，字词之间实际上是存在相关性的，比如近义词之间的相关性就比不是近义词的两个词之间的相关性或者说相似性要高一些，而one-hot的编码并不能反映出这种相关性。\n",
    "\n",
    "为了解决one-hot编码的缺陷，研究人员便发明了词嵌入技术，也就是Embedding。Embedding技术通过将词映射为一个n维的实向量，维度低于one-hot的编码，这样可以获得更高效，更紧凑的编码标示。此外，Embedding获得的向量可以反应词之间的相关性或者相似性，含义接近或者说相关的词，会被映射到空间中相临近的位置。\n",
    "可以直接使用预训练好的Embedding，也可以自定义一个可训练的Embedding，在训练过程中同神经网络的其它部分一起更新。\n",
    "### 2.2 什么是位置编码（Positional encoding）\n",
    "使用RNN结构的模型处理序列数据时，要获得第k个输出，必须先获得前面所有的k-1个输出和隐状态, 第k-1个隐状态编码了前k-1个token的信息。Transformer为了能更好地进行并行计算，抛弃了顺序计算的RNN结构，通过自注意力机制对每个词向量进行重编码，使每个词向量都包含句子中的其它词的信息，但是自注意力机制并不能建模token在序列中的位置信息，比如对\"我爱你\"和\"你爱我\"进行自注意力机制后，编码后的词向量是完全相同的，而显然我们应该希望这两者编码后的结果是不同的。    \n",
    "为了补充token在序列中的位置信息，Transformer引入了位置编码（positional encoding)。\n",
    "Transformer论文中介绍的位置编码方法如下：\n",
    "$PE(pos, 2i)=sin(pos/1000^{2i/dmodel})$    \n",
    "$PE(pos, 2i+1)=cos(pos/1000^{2i/dmodel})$  \n",
    "\n",
    "其中    \n",
    "+ pos表示token在序列中的位置索引,对于语言来说，即表示这是第几个单词\n",
    "+ i表示是词嵌入向量的第几个维度    \n",
    "\n",
    "对于嵌入向量的偶数位，使用变周期的正弦函数来进行编码，对于嵌入向量的奇数位，使用变周期的余弦函数来进行编码。    \n",
    "需要注意的是，使用变周期的三角函数来做位置编码，并不是唯一的方式，只是论文中恰好选用了这种方式。学习此处时，可不必拘泥于计算细节。\n",
    "而更应理解使用位置编码的原因和一个好的位置编码函数应该具有什么特点。\n",
    "一般来说，一个好的位置编码函数应该具有以下两个特点：\n",
    "+ 数量级稳定且有界，不会过大或者过小\n",
    "+ 应该是连续且平滑渐变的，能反映位置间的相对距离\n",
    "+ 编码空间够大，能够避免编码碰撞\n",
    "\n",
    "不难看出，正弦函数和余弦函数都满足前两个要求，为了满足第三个特性，Transformer中设计了变周期的三角函数和奇偶交替的方式，从而避免了编码重复\n",
    "\n",
    "### 2.3 什么是自注意力机制\n",
    "$Attention(Q, K, V) = softmax(QK^T / \\sqrt d_k) * V$    \n",
    "\n",
    "其中\n",
    "\n",
    "+ Q 表示查询向量（Query），\n",
    "+ K 表示键向量（Key），\n",
    "+ V 表示值向量（Value），\n",
    "+ softmax 表示softmax函数，\n",
    "+ d_k 表示查询和键的维度。\n",
    "\n",
    "上面就是大名鼎鼎的自注意力机制的计算公式。从形式上来看，就是两个矩阵乘的结果除以一个常数，然后经过softmax函数，再与另一个矩阵相乘得到最终的结果。    \n",
    "在深入理解自注意力的含义之前，我们先复习一些关于向量的基础知识。\n",
    "在介绍Embedding时，我们提到embedding之后的向量可以反应词之间的相关性。如何度量这种相关性呢? 在机器学习和自然语言处理等领域，我们常常使用向量之间的内积（或点积）来度量它们的相关度或相似度。这是因为内积在某种程度上可以捕捉到向量之间的几何关系和方向性。当两个向量的夹角 θ 接近于0度时，即它们趋向于同一方向，内积的结果将趋近于两个向量模的乘积，表示它们的相似度较高。而当夹角 θ 接近于90度时，即它们趋向于正交（垂直）的关系，内积的结果将趋近于0，表示它们的相似度较低。请先记住这个结论，<font color=##FF0000>两个向量的点积可以表示两个向量的相似度</font>    \n",
    "下面我将举一个具体的例子来解释自注意力机制的原理\n",
    "![embedding](pngs/embedding.png)\n",
    "假设我们有这样一个句子：“我爱你”。每个字可以用一个向量表示，那么整个句子可以用一个（3，5）的矩阵来表示\n",
    "![Q*K^T](pngs/mat_mul.png)\n",
    "我们将这个矩阵与它的转置矩阵做转置乘法，可以得到一个（3，3）的矩阵。如上图所示。矩阵中的结果来自词向量间两两做内积，即整个矩阵描述了词和词之间的相关性。比如第一行的三个数字14.26， 9.2， 13.95，就分别表示了“我与我”，“我与爱”， “我与你”之间的相关性。<font color=##FF0000>注意，公式和实际的Transformer实现中，还需要给得到的这个矩阵除以$\\sqrt {dmodel}$, 目的是防止点积后的结果过大导致梯度不稳定，此处演示简化了这一步，没有除以$\\sqrt {dmodel}$</font> \n",
    "\n",
    "在每一行内使用softmax函数进行处理，就可以得到一个归一化之后的相似度矩阵。\n",
    "![softmax_res](pngs/softmax_res.png)\n",
    "知道了词与词之间的相关性之后，就可以对词向量进行加权融合了。  \n",
    "![output](pngs/self_attention_output.png)\n",
    "比如\"我\"，原本的词向量为(0.5, 0.1, 1, 2, 3)，和句子中所有其它词根据相似度矩阵中的权重进行融合后得到的新的词向量为(0.628, 0.2755, 1.429, 2, 2.781)。融合后，每个词向量不仅包含了这个词本身的含义，还编码了句子中其它字词的含义，再加上前面介绍的位置编码，就获得了句子中完整的上下文信息(context)。\n",
    "### 2.4 什么是多头自注意力\n",
    "上面我们介绍了自注意力的计算方式和它的含义，可以看到，自注意力没有任何参数，在embedding确定的情况下，自注意力的结果也是确定的。为了让自注意力机制能够有学习能力，我们可以为它加上参数，使用线性层对Q,K,V矩阵进行重新映射\n",
    "即$Q=QW^Q$,$K=KW^K$,$V=VW^V$。\n",
    "为了让自注意力机制的学习能力更强，Transformer中没有只使用一个线性层来投影，而是将Q,K,V都分成N份，每份都有自己的投影层，然后独立计算self attention,然后再把每一份的输出汇总在一起，这就是多头注意力的由来。在Transformer的论文中，设置N=8.具体实现时，通过reshape和transpose操作将形状如（batch,seq_length, dmodel）的矩阵调整为（batch, N, seq_length, dmodel//N），然后在最后两个维度上进行矩阵乘计算自注意力。使用这样的多头注意力而不是单个的注意力，可以提高模型的表达能力。\n",
    "### 2.5 什么是Feed forward network\n",
    "所谓的Feed forward network，其实就是一个两层的全连接网络，其中使用了relu作为激活函数，使用了dropout来缓解过拟合\n",
    "### 2.6 关于网络结构的其它小细节\n",
    "Transformer的网络中还有一些其它小细节需要注意\n",
    "\n",
    "Transformer中实际上存在三种不同的多头自注意力，一种是编码器中的用于处理输入的自注意力，计算时输入的Q,K,V完全相同，都是上一层的输出，另一种是解码器中用于处理shifted right序列的masked自注意力，计算时输入的QKV也完全相同，第三种是解码器中用于融合编码器的输出的自注意力层，称为交叉自注意力层，其中的Q来自解码器的上一层输出，K和V都来自编码器的输出。\n",
    "\n",
    "\n",
    "在处理语言或者其它序列数据时，由于每条序列的长度各异，在打包成batch时，必须对其中较短的序列进行补齐,也就是padding,补齐时会选择一个特定的值作为padding值，比如0，而在神经网络计算时，我们希望网络能够忽略这些padding值对应的信息，不被这些占位符影响。\n",
    "\n",
    "此外，Transformer中还存在另外一种mask，在使用解码器来做自回归预测时，预测第k个输出时，只能看到前面k-1个已经预测的输出。而在训练时，由于我们使用了shifted right,相当于网络可以看到整个句子，为了避免网络学会作弊，需要进行掩码处理，即句子中的第i个字词，在进行自注意力融合时，只能和自己所处位置之前的其它字符进行融合，而不能和自己之后的字符进行融合。\n",
    "\n",
    "推理和训练时候的区别\n",
    "训练时候为了避免网络的预测值出现过大的偏差，影响后续token的预测，故将ground truth右移得到shifted right，然后将shifted right作为解码器的输入，相当于直接使用正确的标签来进行自回归，这称为teacher forcing. 在测试时，没有正确的标签了，所以只能使用网络预测的前k-1个字符去预测第k个字符，这才是标准的自回归操作，特别的，在预测第一个字符时，由于前面还没有产生预测值，所以需要放置一个起始字符来作为解码器的输入。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42225b65",
   "metadata": {},
   "source": [
    "# 3. 从零构建Transformer\n",
    "![Transformer模型结构图](pngs/architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4aec13b",
   "metadata": {},
   "source": [
    "### 3.1 环境配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dad064",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install tokenizers\n",
    "!pip install spacy\n",
    "!pip3 install torch torchvision torchaudio torchtext --index-url https://download.pytorch.org/whl/cu117\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download pt_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94fe3ba-bf62-4030-809a-73e5db4bbf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的模块\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch\n",
    "import math\n",
    "from collections import Counter, OrderedDict\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datasets import load_dataset\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import log_softmax\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torchtext.vocab import vocab\n",
    "from functools import partial\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9585e5a5-ebe4-4db2-9646-d9e73017083c",
   "metadata": {},
   "source": [
    "### 3.2 Embedding层的实现\n",
    "embedding层的作用是将由字典中的位置索引表示的词映射成一个n维的向量，使用这个向量来表示单词。    \n",
    "embedding层有两个参数：\n",
    "+ vocal_size表示词典的长度\n",
    "+ d_model表示向量的维度，在Transformer论文中，d_model被设置为512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b434808-8b03-4c16-9385-83f766379e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LayerNorm层的实现，也可以直接使用nn.LayerNorm\n",
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layernorm module (See citation for details).\"\n",
    "\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
    "\n",
    "\n",
    "# 词嵌入层\n",
    "# 将token在词典中的id映射为一个d_model维的向量\n",
    "class Embedder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: tokenlized sequence\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # 乘以一个较大的系数，放大词嵌入向量，\n",
    "        # 希望与位置编码向量相加后，词嵌入向量本身的影响更大\n",
    "        return self.embed(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea3e85a-53eb-4f6a-8990-f2dbbd7e954e",
   "metadata": {},
   "source": [
    "### 3.3 位置编码层的实现\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183a2b02-64a5-4950-b857-83d7acb24f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 位置编码。这段代码的实现是抄的\n",
    "# 预先计算好所有可能的位置编码，然后直接查表就能得到\n",
    "# 注意维度顺序\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 1000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.permute(1, 0, 2)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # X的形状为 (batch_size, seq_length, d_model)\n",
    "        x = x + self.pe[:, :x.size(1), :].requires_grad_(False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0439a063-db72-4b85-88cd-186dcfd17259",
   "metadata": {},
   "source": [
    "### 3.4 多头注意力Multi head attention层的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5036665-22d5-4f19-959f-c9db5e8f2dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多头自注意力\n",
    "# 这段代码的实现也是抄的现成的。奇文共欣赏，疑义相与析\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, head_number, d_model):\n",
    "        \"\"\"\n",
    "        :param head_number: 自注意力头的数量\n",
    "        :param d_model: 隐藏层的维度\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.h = head_number\n",
    "        self.d_model = d_model\n",
    "        self.dk = d_model // head_number\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.output = nn.Linear(d_model, d_model)\n",
    "        self.softmax = nn.Softmax(-1)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def head_split(self, tensor, batch_size):\n",
    "        # 将(batch_size, seq_len, d_model) reshape成 (batch_size, seq_len, h, d_model//h)\n",
    "        # 然后再转置第1和第2个维度，变成(batch_size, h, seq_len, d_model/h)\n",
    "        return tensor.view(batch_size, -1, self.h, self.dk).transpose(1, 2)\n",
    "\n",
    "    def head_concat(self, similarity, batch_szie):\n",
    "        # 恢复计算注意力之前的形状\n",
    "        return similarity.transpose(1, 2).contiguous() \\\n",
    "            .view(batch_szie, -1, self.d_model)\n",
    "\n",
    "    def cal_attention(self, q, k, v, mask=None):\n",
    "        \"\"\"\n",
    "        论文中的公式 Attention(K,Q,V) = softmax(Q@(K^T)/dk**0.5)@V\n",
    "        ^T 表示矩阵转置\n",
    "        @ 表示矩阵乘法\n",
    "        \"\"\"\n",
    "        similarity = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.dk)\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "            # 将mask为0的位置填充为绝对值非常大的负数\n",
    "            # 这样经过softmax后，其对应的权重就会非常接近0, 从而起到掩码的效果\n",
    "            similarity = similarity.masked_fill(mask == 0, -1e9)\n",
    "        similarity = self.softmax(similarity)\n",
    "        similarity = self.dropout(similarity)\n",
    "\n",
    "        output = torch.matmul(similarity, v)\n",
    "        return output\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \"\"\"\n",
    "        q,k,v即自注意力公式中的Q,K,V，mask表示掩码\n",
    "        \"\"\"\n",
    "        batch_size, seq_length, d = q.size()\n",
    "        q = self.q_linear(q)\n",
    "        k = self.k_linear(k)\n",
    "        v = self.v_linear(v)\n",
    "        # 分成多个头\n",
    "        q = self.head_split(q, batch_size)\n",
    "        k = self.head_split(k, batch_size)\n",
    "        v = self.head_split(v, batch_size)\n",
    "        similarity = self.cal_attention(q, k, v, mask)\n",
    "        # 合并多个头的结果\n",
    "        similarity = self.head_concat(similarity, batch_size)\n",
    "\n",
    "        # 再使用一个线性层， 投影一次\n",
    "        output = self.output(similarity)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898024aa-a488-4f26-ab87-3bfc3b4d2051",
   "metadata": {},
   "source": [
    "### 3.5 前馈神经网络的实现\n",
    "不要被名字唬到，前馈神经网络就是全连接神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd03861-98af-4b15-8281-32a3eb1a2987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 论文中的前馈神经网络。实际上就是一个两层的全连接，中间加上个relu和dropout\n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, d_model, dff, dropout=None):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, dff)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(dff, d_model)\n",
    "        if dropout is not None:\n",
    "            self.dropout = nn.Dropout(0.1)\n",
    "        else:\n",
    "            self.dropout = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: 来自多头自注意力层的输出\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        if self.dropout is not None:\n",
    "            x = self.dropout(x)\n",
    "        output = self.fc2(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfa2a56-96f6-4bc8-95b4-d8e46b38ada6",
   "metadata": {},
   "source": [
    "### 3.6 编码器层的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4d3907-f05a-435d-b007-9742a744de88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 编码器层。\n",
    "# 每个编码器层由两个sublayer组成，即一个多头注意力层和一个前馈网络\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, head_number, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # mha\n",
    "        self.mha = MultiHeadAttention(head_number, d_model)\n",
    "        self.norm1 = LayerNorm(d_model)\n",
    "\n",
    "        # mlp\n",
    "        self.mlp = FeedForwardNetwork(d_model, d_ff)\n",
    "        self.norm2 = LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x2 = self.norm1(x)\n",
    "        \n",
    "        y = self.dropout1(self.mha(x2, x2, x2, mask))\n",
    "        # 注意残差连接是和norm之前的输入相加，norm之后的不在一个数量级\n",
    "        y = y + x\n",
    "        \n",
    "        y2 = self.norm2(y)\n",
    "        y2 = self.dropout2(self.mlp(y2))\n",
    "        y2 = y + y2\n",
    "\n",
    "        return y2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0bcf2b-2fad-422f-8e41-b1f84a05ed32",
   "metadata": {},
   "source": [
    "### 3.7 编码器的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cdd0d2-771d-4ad3-bb44-9dbf7c451c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 编码器部分\n",
    "# 编码器就是N个编码器层堆叠起来。论文中为6个编码器层\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, stack=6, multi_head=8, d_model=512, d_ff=2048):\n",
    "        \"\"\"\n",
    "        :param stack: 堆叠多少个编码器层\n",
    "        :param multi_head: 多头注意力头的数量\n",
    "        :param d_model: 隐藏层的维度\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.encoder_stack = []\n",
    "        for i in range(stack):\n",
    "            encoder_layer = EncoderLayer(multi_head, d_model, d_ff)\n",
    "            self.encoder_stack.append(encoder_layer)\n",
    "        self.encoder = nn.ModuleList(self.encoder_stack)\n",
    "        self.norm = LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        for encoder_layer in self.encoder:\n",
    "            x = encoder_layer(x, mask)\n",
    "        x = self.norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f460e2-6332-496b-a096-a03055910d75",
   "metadata": {},
   "source": [
    "### 3.8 解码器层的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d3bf91-9b2d-47a1-b088-7dc7af342397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解码器层\n",
    "# 一个解码器层由三个sublayer组成，即一个masked自注意力层，一个cross自注意力层和一个前馈网络层\n",
    "# cross自注意力层意思是q,k,v分别来自编码器和解码器\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, head_number, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # shifted right self attention layer\n",
    "        self.mha1 = MultiHeadAttention(head_number, d_model)\n",
    "\n",
    "        # cross attention\n",
    "        self.mha2 = MultiHeadAttention(head_number, d_model)\n",
    "\n",
    "        self.norm1 = LayerNorm(d_model)\n",
    "        self.norm2 = LayerNorm(d_model)\n",
    "        self.norm3 = LayerNorm(d_model)\n",
    "\n",
    "        self.mlp = FeedForwardNetwork(d_model, d_ff, 0.1)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q, k, v, src_mask, tgt_mask):\n",
    "        # 注意第一个注意力层的qkv都是同一个\n",
    "        x2 = self.norm1(q)\n",
    "        y = self.mha1(x2, x2, x2, tgt_mask)\n",
    "        y = self.dropout1(y)\n",
    "        \n",
    "        # 注意残差连接是和norm之前的输入相加，norm之后的不在一个数量级\n",
    "        y = y + q\n",
    "        \n",
    "        y2 = self.norm2(y)\n",
    "        \n",
    "        # 第二个自注意力层的k和v是encoder的输出\n",
    "        y2 = self.mha2(y2, k, v, src_mask)\n",
    "        y2 = self.dropout2(y2)\n",
    "        y2 = y + y2\n",
    "        \n",
    "        y3 = self.norm3(y2)\n",
    "        y3 = self.mlp(y3)\n",
    "        y3 = self.dropout3(y3)\n",
    "        y3 = y2 + y3\n",
    "\n",
    "        return y3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afd8371-efe3-404f-81f2-81aaaa5e714d",
   "metadata": {},
   "source": [
    "### 3.9 解码器的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd5bb9a-d26e-4b9d-9a63-3b81d3251209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解码器\n",
    "# 解码器就是N个解码器层堆叠起来\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, stack=6, head_number=8, d_model=512, d_ff=2048):\n",
    "        super().__init__()\n",
    "        self.decoder_stack = []\n",
    "        for i in range(stack):\n",
    "            self.decoder_stack.append(DecoderLayer(head_number, d_model, d_ff))\n",
    "        self.decoder_stack = nn.ModuleList(self.decoder_stack)\n",
    "        self.norm = LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, output_from_encoder, src_mask, tgt_mask):\n",
    "        for decoder_layer in self.decoder_stack:\n",
    "            x = decoder_layer(x, output_from_encoder, output_from_encoder, src_mask, tgt_mask)\n",
    "        x = self.norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7b9249-7972-4660-8c06-73a2876f261f",
   "metadata": {},
   "source": [
    "### 3.10 最终的变形金刚(Transformer)\n",
    "将编码器和解码器组合起来，再加上输入和最终的输出层，就可以得到一个完整的Transformer模型了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0dbfe9-2caf-443a-843d-94b8c0b6f506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最终的变形金刚\n",
    "# 汽车人出发！！！！！！！！！！！！！！！！\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 src_voc_size,\n",
    "                 target_voc_size,\n",
    "                 stack_number=6,\n",
    "                 d_model=512,\n",
    "                 h=8,\n",
    "                 d_ff=2048):\n",
    "        super().__init__()\n",
    "        self.input_embedding_layer = Embedder(src_voc_size, d_model)\n",
    "        self.input_pe = PositionalEncoding(d_model)\n",
    "\n",
    "        self.output_embedding_layer = Embedder(target_voc_size, d_model)\n",
    "        self.output_pe = PositionalEncoding(d_model)\n",
    "\n",
    "        self.encoder = Encoder(stack_number, h, d_model, d_ff)\n",
    "        self.decoder = Decoder(stack_number, h, d_model, d_ff)\n",
    "        self.final_output = nn.Linear(d_model, target_voc_size)\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        x = self.input_embedding_layer(src)\n",
    "        x = self.input_pe(x)\n",
    "        output_from_encoder = self.encoder(x, src_mask)\n",
    "        return output_from_encoder\n",
    "\n",
    "    def decode(self, output_from_encoder, shifted_right, src_mask, tgt_mask):\n",
    "        shifted_right = self.output_embedding_layer(shifted_right)\n",
    "        shifted_right = self.output_pe(shifted_right)\n",
    "\n",
    "        decoder_output = self.decoder(shifted_right, output_from_encoder, src_mask, tgt_mask)\n",
    "\n",
    "        output = self.final_output(decoder_output)\n",
    "        return output\n",
    "\n",
    "    def forward(self, x, shifted_right, src_mask, tgt_mask):\n",
    "        x = self.input_embedding_layer(x)\n",
    "        x = self.input_pe(x)\n",
    "\n",
    "        output_from_encoder = self.encoder(x, src_mask)\n",
    "\n",
    "        shifted_right = self.output_embedding_layer(shifted_right)\n",
    "        shifted_right = self.output_pe(shifted_right)\n",
    "\n",
    "        decoder_output = self.decoder(shifted_right, output_from_encoder, src_mask, tgt_mask)\n",
    "\n",
    "        output = log_softmax(self.final_output(decoder_output), dim=-1)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "def build_model(src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
    "    model = TransformerModel(src_vocab, tgt_vocab, N, d_model, h, d_ff)\n",
    "    # 权重初始化\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# 简单测试。\n",
    "# 验证网络的结构的连通性和输入输出的维度\n",
    "def basic_test():\n",
    "    a = np.random.randint(0, 100, size=(1, 10)).astype(np.int32)\n",
    "    b = torch.from_numpy(a)\n",
    "\n",
    "    a2 = np.random.randint(0, 100, size=(1, 12)).astype(np.int32)\n",
    "    b2 = torch.from_numpy(a2)\n",
    "    model = build_model(100, 120)\n",
    "    print(model)\n",
    "    output = model(b, b2, None, None)\n",
    "    print(output.shape)\n",
    "basic_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7edab40-e3c1-43f0-b483-e7e93814a43b",
   "metadata": {},
   "source": [
    "# 4. 训练Transformer用作机器翻译"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc224ea",
   "metadata": {},
   "source": [
    "## 4.1 数据集加载\n",
    "ted_hrlr/pt_to_en是一个葡萄牙语与英语的文本数据集，其中包含了大约五万组<葡萄牙语-英语>的句子对。方便起见，使用Huggingface的dataset来读取这个数据集,如果下载失败，请尝试使用代理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3ff91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('ted_hrlr', name='pt_to_en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d9e9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 简单check\n",
    "train_data = dataset['train']['translation']\n",
    "print(type(train_data))\n",
    "print(\"训练集数量为:{}\\n\".format(len(train_data)))\n",
    "for data in train_data[:5]:\n",
    "    en_seq = data['en']\n",
    "    pt_seq = data['pt']\n",
    "    print('英语句子为:\\n{}'.format(en_seq))\n",
    "    print('葡萄牙语句子为:\\n{}'.format(pt_seq))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1256b22d",
   "metadata": {},
   "source": [
    "## 4.2 Tokenize以及字典\n",
    "句子就是一个由一组词语按一定顺序组成的，在计算机中，词语和句子都可以由字符串来进行表示。而神经网络是无法直接处理字符串的，因此，有必要对单词进行编码，使用数字来表示单词。在数字和单词之间建立起映射关系，就是通过tokenize和建立字典来完成的。\n",
    "Tokenize就是将一个句子转化为一组基本的token，token表示最小的语义单位。常用的tokenize的方法根据粒度的大小可以分为单词法，子词法和字符法。\n",
    "将句子中所有的token提取出来之后，就可以为token建立字典，将每个token与一个整数标示的ID一一对应起来，这样就可以获得token的数字化表示，也就可以得到一个句子的数字化表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cb9df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(seqs, tokenizer, k=None):\n",
    "    counter = Counter()\n",
    "    for seq in seqs:\n",
    "        if k is not None:\n",
    "            seq = seq[k]\n",
    "        token = tokenizer(seq)\n",
    "        counter.update(token)\n",
    "    sorted_by_freq_tuples = sorted(counter.items(),\n",
    "                                   key=lambda x: x[1],\n",
    "                                   reverse=True)\n",
    "    ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
    "    # 指定特殊字符，特殊字符会被放在字典的起始未知，比如'<pad>'的索引就是0\n",
    "    voc = vocab(ordered_dict, specials=['<pad>', '<unk>', '<bos>', '<eos>'])\n",
    "    return voc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a75661",
   "metadata": {},
   "source": [
    "## 4.3 构建数据集和DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e40354",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(data_batch):\n",
    "    pt_batch, en_input_batch, en_label_batch = [], [], []\n",
    "    for pt, eni, enl in data_batch:\n",
    "        pt_batch.append(pt)\n",
    "        en_input_batch.append(eni)\n",
    "        en_label_batch.append(enl)\n",
    "\n",
    "    pt_batch = pad_sequence(pt_batch, padding_value=0, batch_first=True)\n",
    "    # 使用0来补齐\n",
    "    en_input_batch = pad_sequence(en_input_batch, padding_value=0, batch_first=True)\n",
    "    en_label_batch = pad_sequence(en_label_batch, padding_value=0, batch_first=True)\n",
    "\n",
    "    return pt_batch, en_input_batch, en_label_batch\n",
    "\n",
    "\n",
    "def seq_to_index(seq, tokenizer, voc):\n",
    "    # 添加起始字符和结束字符\n",
    "    indexs = [voc['<bos>']] + [voc[token] for token in tokenizer(seq)] + [voc['<eos>']]\n",
    "    return torch.tensor(indexs, dtype=torch.int64)\n",
    "\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, dataset, en_voc, pt_voc, en_tokenizer, pt_tokenizer):\n",
    "        self.dataset = dataset\n",
    "        self.en_tokenizer = en_tokenizer\n",
    "        self.pt_tokenizer = pt_tokenizer\n",
    "        self.en_voc = en_voc\n",
    "        self.pt_voc = pt_voc\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        en_seq, pt_seq = self.dataset[item]['en'], self.dataset[item]['pt']\n",
    "\n",
    "        # seq to token list. start and end token added\n",
    "        en_tensor = seq_to_index(en_seq, self.en_tokenizer, self.en_voc)\n",
    "        pt_tensor = seq_to_index(pt_seq, self.pt_tokenizer, self.pt_voc)\n",
    "\n",
    "        ground_truth = en_tensor[1:]  # drop the start token\n",
    "        shifted_right = en_tensor[:-1]  # drop the end token\n",
    "        return pt_tensor, shifted_right, ground_truth\n",
    "\n",
    "    \n",
    "def build_dataloader(batch_size, cache_dir=None):\n",
    "    # 读取《葡萄牙语-英语》翻译数据集\n",
    "    dataset = load_dataset('ted_hrlr', name='pt_to_en', cache_dir=cache_dir)\n",
    "\n",
    "    # 获取训练集切分和验证集切分\n",
    "    train_data = dataset['train']['translation']\n",
    "    val_data = dataset['validation']['translation']\n",
    "\n",
    "    # 分词器\n",
    "    en_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "    pt_tokenizer = get_tokenizer('spacy', language='pt_core_news_sm')\n",
    "\n",
    "    # 构建字典索引\n",
    "    en_voc = build_vocab(train_data+val_data, en_tokenizer, k='en')\n",
    "    pt_voc = build_vocab(train_data+val_data, pt_tokenizer, k='pt')\n",
    "\n",
    "    # 创建dataloader\n",
    "    train_ds = TranslationDataset(train_data, en_voc, pt_voc, en_tokenizer, pt_tokenizer)\n",
    "    val_ds = TranslationDataset(val_data, en_voc, pt_voc, en_tokenizer, pt_tokenizer)\n",
    "    train_iter = DataLoader(train_ds, batch_size=batch_size,\n",
    "                            shuffle=True, collate_fn=generate_batch)\n",
    "    # 将验证集的batch_size设置为1\n",
    "    valid_iter = DataLoader(val_ds, batch_size=1,\n",
    "                            shuffle=False, collate_fn=generate_batch)\n",
    "    return train_iter, valid_iter, len(en_voc), len(pt_voc), en_voc\n",
    "\n",
    "\n",
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1).type(\n",
    "        torch.uint8\n",
    "    )\n",
    "    return subsequent_mask == 0\n",
    "\n",
    "\n",
    "# 创建mask\n",
    "def create_mask(src, tgt, pad=0):\n",
    "    src_mask = (src != pad).unsqueeze(-2)\n",
    "    tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "    tgt_mask = tgt_mask & subsequent_mask(tgt.size(-1)).type_as(\n",
    "        tgt_mask.data\n",
    "    )\n",
    "    return src_mask, tgt_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2018d0a6",
   "metadata": {},
   "source": [
    "## 4.5 训练\n",
    "我们将在这个机器翻译数据集上训练60个epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2270e533",
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOBAL_STEP = 1\n",
    "device='cuda'\n",
    "\n",
    "\n",
    "def train_one_step(model, loss_fn, batch_data, \n",
    "                   optimizer, lr_scheduler=None, \n",
    "                   log_writter=None,\n",
    "                   global_step=None):\n",
    "    # 切换到训练模式\n",
    "    model.train()\n",
    "    \n",
    "    # x1为源语言序列，x2为shifted right,label表示目标语言序列，也就是标签\n",
    "    x1, x2, label = batch_data\n",
    "\n",
    "    # 创建掩码\n",
    "    # src掩码的作用是为了忽略padding的字符\n",
    "    # tgt掩码的作用是自回归时候遮蔽当前位置之后的字符\n",
    "    src_mask, tgt_mask = create_mask(x1, x2)\n",
    "\n",
    "    x1 = x1.to(device)\n",
    "    x2 = x2.to(device)\n",
    "    label = label.to(device)\n",
    "    src_mask = src_mask.to(device)\n",
    "    tgt_mask = tgt_mask.to(device)\n",
    "    \n",
    "    # 前向获得输出\n",
    "    pred = model(x1, x2, src_mask, tgt_mask)\n",
    "\n",
    "    label = label.to(torch.long)\n",
    "    pred = pred.contiguous().view(-1, pred.size(-1))\n",
    "    label = label.contiguous().view(-1, )\n",
    "    \n",
    "    # 计算损失\n",
    "    loss = loss_fn(pred, label)\n",
    "    \n",
    "    # 更新权重\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if lr_scheduler is not None:\n",
    "        lr_scheduler.step()\n",
    "    if log_writter is not None:\n",
    "        log_writter.add_scalar('train_loss', loss, global_step)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def rate(step, model_size=512, warmup=4000):\n",
    "    # 学习率调度\n",
    "    if step == 0:\n",
    "        step = 1\n",
    "    return (\n",
    "            model_size ** (-0.5) * min(step ** (-0.5), step * warmup ** (-1.5))\n",
    "    )\n",
    "\n",
    "\n",
    "def train():\n",
    "    # 超参数设置\n",
    "    batch_size = 48\n",
    "    epochs = 60\n",
    "    checkpoint_interval = 1\n",
    "\n",
    "    # 创建数据输入管道\n",
    "    training_data, val_data, tgt_voc_size, src_voc_size, tgt_voc = build_dataloader(batch_size)\n",
    "\n",
    "    # 构建模型\n",
    "    model = build_model(src_voc_size, tgt_voc_size, N=6, d_ff=1024)\n",
    "    model.to(device)\n",
    "\n",
    "    # 优化器，注意此处的lr设置比较大，作为一个基础学习率，经过调度器缩放后，数量级会减小\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=1, betas=(0.9, 0.98), eps=1e-9\n",
    "    )\n",
    "\n",
    "    lr_scheduler = LambdaLR(\n",
    "        optimizer=optimizer, lr_lambda=lambda step: rate(step)\n",
    "    )\n",
    "\n",
    "    # 交叉熵损失函数，使用label smoothing， 忽略标签中的0, 0表示补齐占位符\n",
    "    loss_fn = partial(F.cross_entropy, ignore_index=0, label_smoothing=0.1)\n",
    "\n",
    "#     logger = SummaryWriter('./logs/')\n",
    "    logger = None\n",
    "    # 训练Loop\n",
    "    global GLOBAL_STEP\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        for step, batch_data in enumerate(training_data):\n",
    "            # 在一个batch上训练\n",
    "            loss = train_one_step(model, loss_fn, batch_data, optimizer, lr_scheduler, log_writter=logger,\n",
    "                                  global_step=GLOBAL_STEP)\n",
    "            GLOBAL_STEP += 1\n",
    "            if step % 200 == 0:\n",
    "                print(\"epoch {}/{}  step:{}  loss:{:.3f}\".format(epoch, epochs, step, loss))\n",
    "        if epoch % checkpoint_interval == 0:\n",
    "            # 周期性保存最新的模型\n",
    "            torch.save(model.state_dict(), 'latest_ckpt.pth')\n",
    "    \n",
    "    if logger is not None:\n",
    "        logger.flush()\n",
    "        logger.close()\n",
    "    torch.save(model.state_dict(), 'transformer.pth')\n",
    "\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d403e79",
   "metadata": {},
   "source": [
    "## 5. 测试\n",
    "注意在训练时候，通过舍弃掉ground truth序列的第一个字符得到shifted right，将得到的序列作为解码器的输入。    \n",
    "而在测试时候，是没有ground truth序列的，所以需要一个单词一个单词地进行生成，生成第k个单词时，总是将前面k-1个已经生成的单词作为输入，这就是自回归的含义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d381820",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.zeros(1, 1).fill_(start_symbol).type_as(src.data)\n",
    "    for i in range(max_len - 1):\n",
    "        out = model.decode(\n",
    "            memory, ys, src_mask, subsequent_mask(ys.size(1)).type_as(src.data)\n",
    "        )[0]\n",
    "        prob = F.softmax(out, dim=-1)\n",
    "        # 每次取预测的最后一个字符\n",
    "        next_word = torch.argmax(prob, dim=-1)[-1].data\n",
    "        ys = torch.cat(\n",
    "            [ys, torch.zeros(1, 1).type_as(src.data).fill_(next_word)], dim=1\n",
    "        )\n",
    "    return ys\n",
    "\n",
    "\n",
    "def token2text(tokens, voc_itos, eos=None):\n",
    "    res = []\n",
    "    for i, token in enumerate(tokens):\n",
    "        c = voc_itos[token]\n",
    "        if eos is not None:\n",
    "            if c == eos:\n",
    "                break\n",
    "        res.append(c)\n",
    "    return ' '.join(res)\n",
    "\n",
    "\n",
    "def test():\n",
    "    device = 'cuda'\n",
    "    # dataloader\n",
    "    batch_size = 1\n",
    "    training_data, val_data, tgt_voc_size, src_voc_size, tgt_voc = build_dataloader(batch_size)\n",
    "    # 构建模型\n",
    "    model = build_model(src_voc_size, tgt_voc_size, N=6, d_ff=1024)\n",
    "    # 加载训练好的模型\n",
    "    model.load_state_dict(torch.load('transformer.pth'))\n",
    "    model.to(device)\n",
    "    \n",
    "    # 切换到测试模式\n",
    "    model.eval()\n",
    "    max_len = 128\n",
    "    itos = tgt_voc.get_itos()\n",
    "    for _ in range(3):\n",
    "        val_data = iter(val_data)\n",
    "        batch_data = next(val_data)\n",
    "        x1, x2, label = batch_data\n",
    "        src_mask, tgt_mask = create_mask(x1, x2)\n",
    "        x1 = x1.to(device)\n",
    "        src_mask = src_mask.to(device)\n",
    "        \n",
    "        # 贪心解码\n",
    "        pred_indexs = greedy_decode(model, x1, src_mask, max_len, 2)[0]\n",
    "        \n",
    "        # 将token ID转换成对应的字符\n",
    "        pred_text = token2text(pred_indexs, itos, eos='<eos>')\n",
    "        label = token2text(label[0], itos, eos='<eos>')\n",
    "        print(\"实际的英语句子为:\")\n",
    "        print(label)\n",
    "        print(\"翻译得到的句子为:\")\n",
    "        print(pred_text)\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c1946d",
   "metadata": {},
   "source": [
    "# 6. 总结\n",
    "至此，你已经成功实现并训练了一个你自己的Transformer模型。\n",
    "如果你想继续学习关于Transformer的知识，或者想了解Transformer及其变体在视觉领域中的应用，下面是一些可供参考的资料:\n",
    "\n",
    "[Annotated Transformer 哈佛大学的Transformer实现  (本文中部分代码借鉴于此)](http://nlp.seas.harvard.edu/annotated-transformer/)\n",
    "\n",
    "[Transformer论文原文](https://arxiv.org/abs/1706.03762)\n",
    "\n",
    "[李沐: Transformer论文精读](https://www.bilibili.com/video/BV1pu411o7BE/?spm_id_from=333.999.0.0&vd_source=7ba4ab07bcd248758aff19a21fc5010b)\n",
    "\n",
    "[Vision Transformer论文原文](https://arxiv.org/abs/2010.11929)\n",
    "\n",
    "[VIT论文精读](https://www.bilibili.com/video/BV15P4y137jb/?spm_id_from=333.999.0.0&vd_source=7ba4ab07bcd248758aff19a21fc5010b)\n",
    "\n",
    "[Swin Transformer论文原文](https://arxiv.org/pdf/2103.14030.pdf)\n",
    "\n",
    "[Swin Transformer论文精读](https://www.bilibili.com/video/BV13L4y1475U/?spm_id_from=333.999.0.0&vd_source=7ba4ab07bcd248758aff19a21fc5010b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
