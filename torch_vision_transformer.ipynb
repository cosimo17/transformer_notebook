{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8a4fdc3",
   "metadata": {},
   "source": [
    "# 1. Vision Transformer介绍\n",
    "![vit](pngs/vit_architecture.png)\n",
    "Transformer模型成为自然语言处理(NLP)领域的事实标准后，许多人都尝试将Transformer结构应用到计算机视觉(CV)领域。这其中最出名的，第一次将Transformer应用在大规模图像识别上的便是谷歌提出的vision Transformer,简称VIT。VIT将一副h*w的图像看成是N个大小为s*s的小网格，$N=(h*w)/(s*s)$. 每个小网格称为一个patch,每个patch可以映射为一个一维的向量来表示。然后将N个patch展开，就将图像转化为了类似自然语言序列的数据格式，可以直接使用Transformer来处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edbac00",
   "metadata": {},
   "source": [
    "# 2. Vision Transformer中的核心概念\n",
    "### 1. 如何将图像作为Transformer的输入\n",
    "Transformer是一种序列模型，用于处理序列数据，而彩色图像对应一个RGB三维像素矩阵，那么如何才能将图像转化为Transformer模型的输入呢?     \n",
    "不难想到，最简单的方法就是将像素矩阵全部展开，展开成1维的序列，但是这样会导致序列长度爆炸，比如对于一个192*192大小的灰度图像，展开后我们将得到一个长为36864的序列，再经过嵌入之后，存储空间和计算成本都将爆炸式增长，显然是不可接受的。    \n",
    "为了降低输入序列的长度，有两种方案，一种是尽量降低输入图像的分辨率，比如可以resize为小分辨率的图像，或者先使用CNN进行特征提取，使用降采样后的特征图作为输入而不是直接使用原始的高分辨率图片作为输入。另一种方案是不再把每一个像素点单独作为一个token,而是把一个n*n大小的窗口内的所有像素当成一个token,Vision Transformer中即采用了这样的方法。\n",
    "### 2. 位置编码\n",
    "与处理自然语言的Transformer一样，VIT同样需要对图像patch进行位置编码，位置编码可以反映patch之间的彼此临近关系。回想一下，Transformer中使用了人工设计的变周期的三角函数来进行位置编码，而VIT中的位置编码不是人工设计的，而是创建了一个可训练的参数，将位置编码交给网络自己去学习。\n",
    "### 3. 特殊的分类token\n",
    "Transformer的编码器的输出的形状与编码器的输入的形状完全一致，都是类似(batch_size, seq_length, d_model)，要进行图像识别，还需要再拼接上全连接层。  \n",
    "\n",
    "如果直接将编码器的输出作为全连接层的输入，会有两个问题,一是全连接层需要输入的维度固定，不同分辨率的图像提取出的patch数量不一样，即seq_length数量不一样，为了满足这一限制，必须将所有图像resize成一样的，这样就不能发挥出Transformer架构本身处理不定长序列的能力，第二个问题是Transformer模型的嵌入层一般都很大，所以seq_length*d_model也将会非常大，导致全连接层的运算量极大，影响训练和推理时的性能。\n",
    "    \n",
    " 为了解决这两个问题，Vision Transformer借鉴了BERT模型中的做法，引入了一个特殊的分类token, 经过编码器后，认为这个特殊的分类token中已经编码了图像的所有信息，分类时，只需要将这一个token对应的向量输入全连接层进行回归即可。这样对于任意长度的序列，都可以保证全连接层的输入是固定的，而且极大地降低了全连接层的输入维度。\n",
    "具体实现中，所谓的分类token,就是在编码器的输入的seq_length维度的0号位置concat了一个维度也为d_model的向量，将输入从(batch_size, seq_length, d_model)变成了(batch_size, 1 + seq_length, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7832e816",
   "metadata": {},
   "source": [
    "# 3. Vision Transformer的实现\n",
    "VIT只使用了Transformer的编码器部分，然后在编码器上拼接了一个MLP来做回归，负责最后的识别。编码器部分的实现和标准Transformer中的实现完全相同，不再赘述。\n",
    "\n",
    "需要注意的是位置编码(PositionalEncoding)的实现，提取图像patch(ImagePatcher)的实现和cls_token的表示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424b0dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 patcher,\n",
    "                 d_model,\n",
    "                 attention_head,\n",
    "                 stack_number,\n",
    "                 dff,\n",
    "                 max_length,\n",
    "                 class_number):\n",
    "        super().__init__()\n",
    "        self.patcher = patcher\n",
    "        self.pe = PositionalEncoding(d_model, max_length)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, d_model), requires_grad=True) # 特殊的分类token\n",
    "        self.encoder = Encoder(stack_number, attention_head, d_model, dff)\n",
    "        self.head = HeadLayer(d_model, class_number, d_model)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pe(self.patcher(x))\n",
    "        cls_token = self.cls_token.expand(x.shape[0], -1, -1) # expand to match the batch dim\n",
    "        x = torch.cat([cls_token, x], dim=1)\n",
    "        x = self.encoder(x)\n",
    "        x = x[:, 0, :]\n",
    "        x = self.norm(x)\n",
    "        output = self.head(x)\n",
    "        return output\n",
    "\n",
    "\n",
    "# 将图像转化为patch的序列，并进行线性映射\n",
    "class ImagePatcher(nn.Module):\n",
    "    def __init__(self, ic, oc, kernel_size, stride):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(ic, oc, kernel_size, stride)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x) # 注意此时的维度顺序为batch_size, channel, h, w\n",
    "        batch_size, channel, h, w = x.shape\n",
    "        x = x.view(batch_size, channel, -1)\n",
    "        x = x.permute(0, 2, 1) # batch_size, seq_length, d_model\n",
    "        return x\n",
    "\n",
    "\n",
    "# 可训练的位置编码\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_length):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(1, max_length, d_model)\n",
    "        pe = nn.Parameter(pe, requires_grad=True)\n",
    "        self.pe = pe\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.shape[1], :]\n",
    "        return x\n",
    "\n",
    "\n",
    "# 前馈神经网络\n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, ic, hidden, dropout=None):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(ic, hidden)\n",
    "        self.linear2 = nn.Linear(hidden, ic)\n",
    "        self.dropout = nn.Dropout(0.1) if dropout is not None else None\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act(self.linear1(x))\n",
    "        if self.dropout is not None:\n",
    "            x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# 分类头。两层全连接中间加个relu\n",
    "class HeadLayer(nn.Module):\n",
    "    def __init__(self, ic, class_number, hidden):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(ic, hidden)\n",
    "        self.act = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden, class_number)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear2(self.act(self.linear1(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "# 多头自注意力\n",
    "# 这段代码的实现也是抄的现成的。奇文共欣赏，疑义相与析\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, head_number, d_model):\n",
    "        \"\"\"\n",
    "        :param head_number: 自注意力头的数量\n",
    "        :param d_model: 隐藏层的维度\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.h = head_number\n",
    "        self.d_model = d_model\n",
    "        self.dk = d_model // head_number\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.output = nn.Linear(d_model, d_model)\n",
    "        self.softmax = nn.Softmax(-1)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def head_split(self, tensor, batch_size):\n",
    "        # 将(batch_size, seq_len, d_model) reshape成 (batch_size, seq_len, h, d_model//h)\n",
    "        # 然后再转置第1和第2个维度，变成(batch_size, h, seq_len, d_model/h)\n",
    "        return tensor.view(batch_size, -1, self.h, self.dk).transpose(1, 2)\n",
    "\n",
    "    def head_concat(self, similarity, batch_szie):\n",
    "        # 恢复计算注意力之前的形状\n",
    "        return similarity.transpose(1, 2).contiguous() \\\n",
    "            .view(batch_szie, -1, self.d_model)\n",
    "\n",
    "    def cal_attention(self, q, k, v, mask=None):\n",
    "        \"\"\"\n",
    "        论文中的公式 Attention(K,Q,V) = softmax(Q@(K^T)/dk**0.5)@V\n",
    "        ^T 表示矩阵转置\n",
    "        @ 表示矩阵乘法\n",
    "        \"\"\"\n",
    "        similarity = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.dk)\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "            # 将mask为0的位置填充为绝对值非常大的负数\n",
    "            # 这样经过softmax后，其对应的权重就会非常接近0, 从而起到掩码的效果\n",
    "            similarity = similarity.masked_fill(mask == 0, -1e9)\n",
    "        similarity = self.softmax(similarity)\n",
    "        similarity = self.dropout(similarity)\n",
    "\n",
    "        output = torch.matmul(similarity, v)\n",
    "        return output\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \"\"\"\n",
    "        q,k,v即自注意力公式中的Q,K,V，mask表示掩码\n",
    "        \"\"\"\n",
    "        batch_size, seq_length, d = q.size()\n",
    "        q = self.q_linear(q)\n",
    "        k = self.k_linear(k)\n",
    "        v = self.v_linear(v)\n",
    "        # 分成多个头\n",
    "        q = self.head_split(q, batch_size)\n",
    "        k = self.head_split(k, batch_size)\n",
    "        v = self.head_split(v, batch_size)\n",
    "        similarity = self.cal_attention(q, k, v, mask)\n",
    "        # 合并多个头的结果\n",
    "        similarity = self.head_concat(similarity, batch_size)\n",
    "\n",
    "        # 再使用一个线性层， 投影一次\n",
    "        output = self.output(similarity)\n",
    "        return output\n",
    "\n",
    "\n",
    "# 编码器层。\n",
    "# 每个编码器层由两个sublayer组成，即一个多头注意力层和一个前馈网络\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, head_number, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # mha\n",
    "        self.mha = MultiHeadAttention(head_number, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "\n",
    "        # mlp\n",
    "        self.mlp = FeedForwardNetwork(d_model, d_ff)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x2 = self.norm1(x)\n",
    "\n",
    "        y = self.dropout1(self.mha(x2, x2, x2, mask))\n",
    "        # 注意残差连接是和norm之前的输入相加，norm之后的不在一个数量级\n",
    "        y = y + x\n",
    "\n",
    "        y2 = self.norm2(y)\n",
    "        y2 = self.dropout2(self.mlp(y2))\n",
    "        y2 = y + y2\n",
    "\n",
    "        return y2\n",
    "\n",
    "\n",
    "# 编码器部分\n",
    "# 编码器就是N个编码器层堆叠起来。论文中为6个编码器层\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, stack=6, multi_head=8, d_model=512, d_ff=2048):\n",
    "        \"\"\"\n",
    "        :param stack: 堆叠多少个编码器层\n",
    "        :param multi_head: 多头注意力头的数量\n",
    "        :param d_model: 隐藏层的维度\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.encoder_stack = []\n",
    "        for i in range(stack):\n",
    "            encoder_layer = EncoderLayer(multi_head, d_model, d_ff)\n",
    "            self.encoder_stack.append(encoder_layer)\n",
    "        self.encoder = nn.ModuleList(self.encoder_stack)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        for encoder_layer in self.encoder:\n",
    "            x = encoder_layer(x, mask)\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def build_model(img_channel, patch_size, d_model, attention_head, stack_number, dff, max_length, class_number):\n",
    "    \"\"\"\n",
    "    :param img_channel: 输入的图像的通道数，对于RGB图像，通道数为3，对于灰度图，通道数为1\n",
    "    :param patch_size: 每个图像patch的大小，比如(2,2)\n",
    "    :param d_model: 嵌入维度\n",
    "    :param attention_head: 多头自注意力的头数\n",
    "    :param stack_number: 自注意力block的数量\n",
    "    :param dff: 前馈神经网络隐藏层的规模\n",
    "    :param max_length: 最大的序列长度，用于创建位置编码\n",
    "    :param class_number: 分类的类别数量\n",
    "    :return: VIT 模型\n",
    "    \"\"\"\n",
    "    patcher = ImagePatcher(img_channel, d_model, patch_size, patch_size)\n",
    "    model = VisionTransformer(patcher, d_model, attention_head, stack_number, dff, max_length, class_number)\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    return model\n",
    "\n",
    "\n",
    "# 简单测试\n",
    "def basic_test():\n",
    "    d_model = 256\n",
    "    attention_head = 4\n",
    "    stack_number = 4\n",
    "    dff = 512\n",
    "    max_length = 200\n",
    "    class_number = 10\n",
    "    model = build_model(1, (2,2), d_model, attention_head, stack_number, dff, max_length, class_number)\n",
    "    print(\"Model Info\")\n",
    "    print(model)\n",
    "\n",
    "    sample = torch.randn(1, 1, 28, 28)\n",
    "    output = model(sample)\n",
    "    print(\"output shape: {}\".format(output.shape))\n",
    "\n",
    "basic_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eadbe8e",
   "metadata": {},
   "source": [
    "# 4. 在MNIST数据集上训练VIT\n",
    "接下来，我们将在MNIST数据集上训练一个小型的Vision Transformer模型，VIT模型有很多的参数可以用来控制模型的规模，包括嵌入层的维度，自注意力block的数量，多头自注意力的数量等，可以根据需求灵活调整，此处仅在MNIST数据集上做演示，所以构建了一个较小的Vision Transformer模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6959ffdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "batch_size = 64\n",
    "device = 'cuda'\n",
    "epochs = 30\n",
    "train_dataset = torchvision.datasets.MNIST(root='data',\n",
    "                                           train=True,\n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "train_data = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "vit_model = build_model(1, (2,2), 128, 4, 4, 1024, 300, 10)\n",
    "vit_model = vit_model.to(device)\n",
    "optimizer = torch.optim.Adam(vit_model.parameters(), lr=1e-4, betas=(0.9, 0.98), eps=1e-9)\n",
    "loss_func = F.cross_entropy\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    loss_sum = 0\n",
    "    acc_sum = 0\n",
    "    n = 0\n",
    "    for step, batch_data in enumerate(train_data):\n",
    "        x, label = batch_data\n",
    "        x = x.to(device)\n",
    "        label = label.to(device)\n",
    "        output = vit_model(x)\n",
    "        loss = loss_func(output, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pred_index = F.softmax(output, dim=1)\n",
    "        pred_index = torch.argmax(pred_index, dim=1)\n",
    "        acc = torch.sum(pred_index == label) / len(label)\n",
    "        loss_sum += loss\n",
    "        acc_sum += acc\n",
    "        n += 1\n",
    "\n",
    "    loss_avg = loss_sum / n\n",
    "    acc_avg = acc_sum / n\n",
    "    print(\"Epoch:{}  Average Loss:{:.3f}  Accuracy:{:.3f}\".format(epoch, loss_avg, acc_avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdff21d",
   "metadata": {},
   "source": [
    "# 5. 总结\n",
    "你已经成功实现并训练了一个Vision Transformer模型，尝试在你自己的数据集上训练并优化它吧。\n",
    "如果你还想继续学习关于Vision Transformer模型的知识，以下是一些可供参考的学习资料\n",
    "+ [Vision Transformer论文精读](https://www.bilibili.com/video/BV15P4y137jb/?spm_id_from=333.999.0.0&vd_source=7ba4ab07bcd248758aff19a21fc5010b)\n",
    "+ [Vision Transformer论文原文](https://arxiv.org/abs/2010.11929)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
